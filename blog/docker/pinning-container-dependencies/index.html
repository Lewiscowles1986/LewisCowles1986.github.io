<!doctype html>
<html itemscope="" itemtype="http://schema.org/WebPage" lang="en">
<head>
    <meta charset="utf-8">
    <link rel="manifest" href="/manifest.webmanifest">
	<meta name="theme-color" content="#333333"/>
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="generator" content="hand-crafted">
	<meta name="description" content="A post about understanding Docker, and pinning container dependencies.">

	<link rel="icon" href="https://www.lewiscowles.co.uk/favicon.ico" type="image/x-icon">
	<link rel="shortcut icon" href="https://www.lewiscowles.co.uk/favicon.ico" type="image/x-icon">

	<link rel="profile" href="http://gmpg.org/xfn/11">

	<title>Web log - Docker - ARGS and ENV, by Lewis Cowles</title>
	<link rel="canonical alternate"  hreflang="en-gb" href="https://www.lewiscowles.co.uk/blog/docker/pinning-container-dependencies/">
	<script type="application/ld+json">
	{
		"@context": "http://schema.org",
		"@type": "Person",
		"name": "Lewis Cowles",
		"url": "http://www.lewiscowles.co.uk",
		"sameAs": [
			"https://www.facebook.com/lewis.r.cowles",
			"https://www.instagram.com/lewiscowles/",
			"https://www.linkedin.com/in/lewiscowles/",
			"https://www.youtube.com/channel/UCl0VmButd52e4GvN8YQh7og",
			"https://github.com/LewisCowles1986",
			"https://gist.github.com/LewisCowles1986"
		]
	}
	</script>
	<script type="application/ld+json">
	{
		"@context": "http://schema.org",
		"@type": "Organization",
		"url": "https://www.lewiscowles.co.uk",
		"logo": "https://www.lewiscowles.co.uk/img/lewiscowles-logo-vector.png"
	}
	</script>
	<link rel="pingback" href="https://webmention.io/www.lewiscowles.co.uk/xmlrpc">
	<link rel="webmention" href="https://webmention.io/www.lewiscowles.co.uk/webmention">
</head>
<body class="page">
	<header>
		<a class="sr-only sr-only-focusable" href="#content" data-skiplink="">Skip to content</a>
		<div class="inner">
			<div class="header-logo tC">
				<img src="/img/lewiscowles-logo-vector.svg" alt="" class="logo" style="width: 12em; width: 12rem;">
				<span class="header-logo-text">Lewis Cowles</span>
			</div>
			<nav>
				<ul id="menu-top-menu" class="menu">
					<li>
						<a href="/about/">About</a>
					</li><li>
						<a href="/blog/">Blog</a>
					</li><li>
						<a href="/">Contact</a>
					</li>
					<li class="toggle-fragmention hidden">
						<a href="#" title="use fragmention (requires JavaScript)">#</a>
					</li>
				</ul>
			</nav>
		</div>


	</header>
	<!-- straight after header -->
	<main id="content">
		<article class="h-entry">
			<h1 class="p-name"><a rel="tag" href="/blog/docker/">Docker</a> - <a class="u-url" href="/blog/docker/pinning-container-dependencies/">Pinning container dependencies</a></h1>
			<time class="post-date dt-published" datetime="2021-03-17">
				<span class="year">2021</span>-<span class="month">03</span>-<span class="day">17</span> 
			</time>
			<div class="e-content">
				<p>Recently I have been receiving a lot of third-party docker setups from third parties.</p>
				<p>Some have been for projects I use or work-on in CI environments. Some are part of client projects. Most have some things in common.</p>
				<ul>
					<li>They use a fraction of the useful features of Docker.</li>
					<li>They can bundle lots of parts into single images.</li>
					<li>They are not explicit in dependency management.</li>
				</ul>
				<p>This post is going to deal with dependency resolution. What to do, and what to try to avoid.</p>
				<h2>The mistakes</h2>
				<h3>Publishing only to latest</h3>
				<p>When people publish docker images only to latest. There is no way back from accidental push. This is by far the easiest problem to overcome. Here are some tagging strategies.</p>
				<ul>
					<li>Deploying based on date-time.</li>
					<li>Deploying based on release version.</li>
					<li>Deploying based on code-name.</li>
					<li>Deploying based on commit ID.</li>
				</ul>
				<p>Of the above I believe that while code-name is the coolest, it is rarely applicable. For the same reason date-time can be too verbose and requires organisational knowledge of a complex web of systems and dates of deployment.</p>
				<p>Commit ID deploys rely on source code access, leaving only release version.</p>
				<h3>Relying on Dockerfile</h3>
				<p>There are two primary ways to use Docker. One is to use a pre-built image, and the other is to build a Dockerfile. Make no mistake. As with any recipe, building a Dockerfile involves more risk than using an available image.</p>
				<ul>
					<li>With a pre-built image, you can export and import a known-working artifact.</li>
					<li>With a Dockerfile, you can understand some of the "how they make the sausage". It is not always important. It can bring false confidence.</li>
				</ul>
				<p>This means a few things, including that if you do not control the source of distribution, you are taking on risk.</p>
				<h3>Single-step builds.</h3>
				<p>After a few versions, docker came up with the concept of using a single Dockerfile, for a single output via multi-stage builds. Quite simply you can build several pieces of software in various (optionally) named stages. You can then use the COPY command to retrieve build-artifacts. This results in a smaller distributable image, but also saves some of the odd and confusing notations that trying to build, install, clean-up and distribute led to. You can now name each step, so intent is clearer.</p>
				<h3>Bringing a Virtual Machine mindset to containers</h3>
				<p>Containers are not the same thing as virtual machines. They follow the same logic, of deploying a general artifact. But that is where it stops.</p>
				<h4>Some things about Virtual Machines</h4>
				<ul>
					<li>They deploy complex environments, consisting of multiple complex parts.</li>
					<li>They duplicate System-level components such as OS Kernel.</li>
					<li>They require specialist hardware support to get near-native performance.</li>
					<li>Limited support via extra tooling for host to guest.</li>
					<li>Guest machine uses isolated permissions, firewall, everything.</li>
				</ul>
				<h4>Contrast with containers</h4>
				<ul>
					<li>Designed to deploy single processes with dependencies, configuration.</li>
					<li>Designed to have limited permissions.</li>
					<li>Designed to support per-instance external configuration management.</li>
					<li>Shares system components.</li>
					<li>Generic hardware support at virtually native speeds.</li>
					<li>Deploys in any Linux environment, meaning a VM for Macintosh and Windows.</li>
				</ul>
				<h2>Good practices</h2>
				<h3>Using environment variables</h3>
				<p>One of the amazing things for me Docker has led to, is a reliance on environment variables, rather than complex configuration files for setting up software. Backing up a little... This was not a Docker invention and I am not pretending it was. But prior to containerization, you had businesses where staff knew about and believed in 12-factor apps, and you had the complicated mess that happens if you do not.</p>
				<p>Prior to 2012 my own apps were usually distributed using complicated configuration files, which had to be manually built, and verified in a labour-intensive process. People just largely did not know better. While not a panacea, environment variables allow applications to at process-launch set information related to running the application. Perhaps you want to test a MySQL database in one container, and Postgres in another. The point is you can deploy one set of code and change the configuration per-unique instance. This has enabled some of the greatest choice and reliability in modern software.</p>
				<h3>Using build arguments</h3>
				<p>I have written about <a href="/blog/docker/args-and-env/">the difference between build-time arguments and environment variables</a> in another posting, so I will keep this brief. Essentially you can pass values which are not part of the image, but only used at build-time, protecting against shipping secrets, and allowing even more customisation.</p>
				<p>One of the most useful things I use build-arguments for is changing a base-image to build from. If I want to know if my Dockerfile will need maintenance between different runtime versions of Python, Ruby, Node, PHP, C or C++ compilers, Java. I Can then easily build a new container passing a different argument. I use this pattern for a self-hosted CI pipeline I started for CODESIGN2. An exercise in increasing my own understanding. Practicing Lean costing and something I really enjoyed. You can also use these to send in secrets such as third-party privileged access, without sending secrets to all consumers.</p>
				<h3>Avoiding process managers, certificate copies, etc.</h3>
				<p>Now this is more advice than a hard and fast rule, but if you find yourself copying certificates, or shoehorning process-managers into Docker, it might not be the right tool for the job. Docker itself is a process manager among other things, so you are really working inside of a "yo-dawg" meme at the point you go this far.</p>
				<p>Similarly, the sidecar pattern of having a complimentary Docker container to manage this complexity, can make troubleshooting easier. What this might look like is an internet service which does communicate using <abbr title="Transport Layer Security">TLS</abbr>. Instead, it might use headers, as it might in a proxy environment.</p>
				<p>Tools like Kubernetes make this coordination between components quite easy, but you can get started using tools such as docker-compose, which allows you to set multiple networks, where perhaps you do not need to juggle ports or get messy to have shared-nothing local architecture, allowing you to spin-up multiple complementary micro-services.</p>
				<h3>Keeping your own registry / export backups</h3>
				<p>The ideal is to have a copy you control of any service that you depend on. If my entire application rests on using a Postgres database. I should likely either pull from a local docker repository, or regularly export from my local pulled image, so that in a disaster situation, I am able to secure the continuity of my business or organisation.</p>
				<p>This topic could get a post of its very own, and perhaps I will publish one, but the mark of a mature Information Technology service provider, is having a degree of control if third parties for whatever reason become unavailable.</p>
				<h3>Maintaining your own Dockerfiles</h3>
				<p>Even if the content of your own Dockerfile is just to pull from an upstream, with no changes to filesystem, or environment-variables. I Recommend keeping a copy.</p>
				<p>I Did not always recommend this. When working at Kalo I actively pushed for us to own less Docker, due to the size of the team and time constraints. Some individuals had a propensity to add unnecessary complexity into Docker images, which added complexity elsewhere. It is still a new technology, so I expect more nuance around this in coming years, but I have been using Docker since 2013.</p>
				<p>Keeping a non-tool specific textual record for building images saying where to pull-from, means we would have easily discoverable dependencies, which can help when dealing with change. It would also mean that we could add commit rules, which are a better tool for stopping people adding complexity.</p>
				<p>In a recent interview task, I was handed a Docker assignment, where pulling from upstream led to a difference in runtime between two containers. One was for running a tool for a language, and the other was to run the program after a build-pipeline. This led to over an hour of "Why is this broken in one but not the other?". If you can save co-workers and or your future self this pain. Please do.</p>
				<h3>Using Kubernetes or docker-compose</h3>
				<p>As much as part of me gets quite upset at the complexity of tooling. Launching docker processes on several machines, checking resource utilization, etc is a complex task and really, it should be the job of a tool such as docker-compose or Kubernetes.</p>
				<p>Personally, I have aired on the swarm, docker-compose camp for years, however I am now coming around to using more modern tooling, even in-house so that I can get access to many types of machine, overloading my laptop, all without standing up, configuring things every time I need them. I have tried manual until it hurts and have confirmed the pain of not using something like this. Especially in microservices or multiple client accounts, which I used to run via Vagrant.</p>
				<h2>Closing</h2>
				<p>Hopefully, I have stayed on topic and talked about docker container dependencies. I have tried to keep this structured as a series of unique headings, so fragmention or the new chrome spec for text highlighting will work.</p>
				<p>I Hope you enjoyed this. I Hope you have learned some thing or been given cause to introspect your own ideas.</p>
				<p>Thank you for reading!</p>
			</div>By <a href="https://www.lewiscowles.co.uk/" class="u-author" rel="author">Lewis Cowles</a>
		</article>
	</main>
	<footer>
	<div class="widget-area">
	<div class="noPad tC">
		<div id="text-2" class="widget widget_text">
			<div class="textwidget">
				<p>
				Copyright Â© 2014-2022 <span class="h-card"><img class="u-x-icon u-logo" src="/img/lewiscowles-logo-vector.svg" alt="" /><a class="p-name u-url u-uid" rel="me" href="https://www.lewiscowles.co.uk/">Lewis Cowles</a></span>, All Rights Reserved.<br>Font Icons By
				<a href="http://fortawesome.github.io/Font-Awesome/" target="_blank" rel="nofollow noopener noreferrer">FontAwesome</a><br>
				Website coded using CSS3, HTML5.<br>
				Images use
				<a href="http://gimp.org" target="_blank" rel="nofollow noopener noreferrer">GIMP</a> &amp; Adobe Creative Suite.
				</p>
			</div>
		</div>
	</div>
	</div>
	<!-- website carbon badge -->
<div id="wcb" class="carbonbadge wcb-d"></div>
<script src="https://unpkg.com/website-carbon-badges@1.1.3/b.min.js" defer></script>
</footer>
	<link href="/css/style.css" rel="stylesheet" media="all">
	
	<script type="text/javascript" src="/js/feature-detect.js" async></script>
</body>
</html>
